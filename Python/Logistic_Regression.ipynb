{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcThZ5VUgaPo"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "XWMypTRbgaPw",
    "outputId": "00f48efe-278a-45e5-9273-2b0381f5ce41"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# importing the dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['Species'] = iris.target\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yYXhO8LgaRV"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,[0,1,2,3]].values\n",
    "y = data.iloc[:,4].values\n",
    "\n",
    "X = X.reshape(-1, 4)\n",
    "X = preprocessing.scale(X) #scale the data so that it is easier to fit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                y, test_size=0.25, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vdkRridglTj"
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test data\n",
    "y_pred = log_reg.predict(X_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zljk3RohgaSo"
   },
   "source": [
    "### Measuring Model Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDGphAhv1lqp"
   },
   "source": [
    "To measure a model's performance, you want to get a sense of overall performance as well as performance for each class. Since the Iris dataset is totally balanced, accuracy is an appropriate measure of performance for the classification overall. An accuracy score is provided with the scikit-learn method as the 'score' attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mMl678ve1uSp",
    "outputId": "3158b09b-e898-4273-d21d-b3bbb7e1babf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = log_reg.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy: {}'.format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9D2wCVXo2XvP"
   },
   "source": [
    "You can also check how the model did for different classes. The confusion matrix shows us that the model found instances of the species Versicolor the hardest to classify correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Ey1Ro-MYgaSq",
    "outputId": "ebb769ef-dd33-44ba-c85b-c5bd6436ce67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            setosa  versicolor  virginica\n",
       "setosa          13           0          0\n",
       "versicolor       0          15          1\n",
       "virginica        0           0          9"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classes = list(iris.target_names)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(conf_mat, columns=classes, index=classes)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxii7pl224vE"
   },
   "source": [
    "Looking at the confusion matrix is one way of inspecting performance in more detail. Looking at the f1 score,  \n",
    "precision and recall are another way. The imperfect average f1 score tells us not all instances were classified perfectly, and the per-class f1 scores tells us which classes were the most problematic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "tVUbnNAM8t6u",
    "outputId": "9a1b1031-22ea-4f79-8ab9-a9259ee847f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n",
      "Hardest class: virginica\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# average f1 score\n",
    "av_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "print(av_f1)\n",
    "\n",
    "# f1 score per class\n",
    "f = f1_score(y_test, y_pred, average=None)\n",
    "lowest_score = min(f)\n",
    "hardest_class = classes[list(f).index(lowest_score)]\n",
    "print('Hardest class:', hardest_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmpIYpFw85px"
   },
   "source": [
    "The precision and recall for that species then tell us more. What what went wrong is that not the model was too strict about what instances could be considered Virginica, or perhaps mistook them for another class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "7Xk0eOu525b4",
    "outputId": "9c4483fc-1740-4757-aafd-b17b4092df27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# precision and recall for virginica  \n",
    "prec = precision_score(y_test == classes.index('virginica'), y_pred == classes.index('virginica'))\n",
    "rec = recall_score(y_test == classes.index('virginica'), y_pred == classes.index('virginica'))\n",
    "\n",
    "print('Precision:', prec)\n",
    "print('Recall:', rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NKQX3-h-BXz"
   },
   "source": [
    "We could use this information to, for example, decide to collect more instances of this species."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
